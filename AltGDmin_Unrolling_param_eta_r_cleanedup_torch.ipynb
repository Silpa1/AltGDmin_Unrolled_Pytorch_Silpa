{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Training code for Learned AltGDmin\n",
        "# Implemented by Silpa Babu\n",
        "# Date: Feb 23, 2024\n",
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "import scipy.sparse.linalg as lina\n",
        "import time\n",
        "\n",
        "## ================Preparations====================\n",
        "#device = torch.device('cuda:0')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "datatype = torch.float64\n",
        "\n",
        "## ================Parameters======================\n",
        "r \t\t\t\t=5\t\t# underlying rank\n",
        "n \t\t\t\t= 200\t# size (num. of rows)\n",
        "q \t\t\t\t= 200\t\t# size (num. of columns)\n",
        "m         = 60    # size(number of measurements)\n",
        "step_initial \t= 0.5\t\t# initial value of step size (eta in the paper)\n",
        "maxIt \t\t\t= 100\t# num. of layers you want to train\n",
        "thr_initial = 0.4\n",
        "## =============Generate Data y_k = A_k U b_k=============\n",
        "def generate_problem(r,n,q):\n",
        "    U0_t, _ \t\t= torch.qr(torch.randn(n,r,dtype = datatype)/math.sqrt(n))\n",
        "    B0_t \t\t    = torch.randn(r,q,dtype = datatype)/math.sqrt(q)\n",
        "    A           = torch.randn(m,n,q,dtype = datatype)/math.sqrt(m)\n",
        "    noise       = torch.randn(n,q,dtype = datatype)/math.sqrt(n * q)\n",
        "    Y0_t        = torch.zeros(m, q,dtype = datatype)\n",
        "    for k in range(q):\n",
        "      Y0_t[:,k]    = torch.matmul(A[:,:,k], torch.matmul(U0_t,B0_t[:,k]))\n",
        "    return U0_t, B0_t, Y0_t, A\n",
        "\n",
        "## ===================LRPCA model===================\n",
        "class MatNet(nn.Module):\n",
        "  # Define parameters to be learned\n",
        "  def __init__(self):\n",
        "    super(type(self),self).__init__()\n",
        "    #self.thr \t\t= [nn.Parameter(Variable(torch.tensor(thr_initial, dtype=datatype), requires_grad=True)) for t in range( maxIt)]\n",
        "    self.step \t\t= [nn.Parameter(Variable(torch.tensor(step_initial, dtype=datatype), requires_grad=True)) for t in range( maxIt)]\n",
        "    self.thr \t\t= [nn.Parameter(torch.tensor(thr_initial, dtype=datatype), requires_grad=True) ]\n",
        "    #self.thr        = [nn.Parameter(Variable(torch.tensor(thr_initial, dtype=datatype), requires_grad=True))]\n",
        "\n",
        "\n",
        "\n",
        "  def lowrank(self, X0, threshold):\n",
        "    X0 = X0.clone()\n",
        "    Ut, St, Vt = torch.linalg.svd(X0)\n",
        "    thres = threshold * St[0]\n",
        "    thres = thres.unsqueeze(-1)\n",
        "    St = F.relu(St - thres)\n",
        "    St = St.float()\n",
        "\n",
        "    Xinit = torch.matmul(torch.matmul(Ut, torch.diag(St)), torch.conj(Vt))\n",
        "    Ut, St, Vt = torch.svd_lowrank(Xinit, niter = 4)\n",
        "    return Ut.double()\n",
        "\n",
        "\n",
        "  def forward(self, Y0_t, U0_t, B0_t, A, num_l):\n",
        "    # Initialization\n",
        "    n , r0 = U0_t.size()\n",
        "    r0, q = B0_t.size()\n",
        "    X0        = torch.zeros(n, q)\n",
        "    for k in range(q):\n",
        "      X0[:,k]    = torch.matmul(A[:,:,k].t(), Y0_t[:,k])\n",
        "    U_t = self.lowrank(X0, self.thr[0])\n",
        "    n , r = U_t.size()\n",
        "    B_t        = torch.zeros(r, q, dtype = datatype)\n",
        "    for k in range(q):\n",
        "      B_t[:,k] = torch.matmul(torch.linalg.pinv(torch.matmul(A[:,:,k], U_t)),Y0_t[:,k]) # Column-wise Least Squares\n",
        "    ## Main Loop\n",
        "    for t in range(1,num_l):\n",
        "      E_t = torch.zeros(n, r, dtype = datatype)\n",
        "      for k in range(q):\n",
        "        E_t = E_t + torch.matmul(A[:,:,k].t(),torch.matmul((Y0_t[:,k] - torch.matmul(A[:,:,k], torch.matmul(U_t, B_t[:,k]))).unsqueeze(1),B_t[:,k].unsqueeze(1).t())) #  Compute Gradient\n",
        "      Unew, _ = torch.qr(U_t + self.step[t] * (E_t)) # Projected Gradient Descent\n",
        "      Bnew        = torch.zeros(r, q, dtype = datatype)\n",
        "      for k in range(q):\n",
        "        Bnew[:,k] = torch.matmul(torch.linalg.pinv(torch.matmul(A[:,:,k], Unew)),Y0_t[:,k]) # Column-wise Least Squares\n",
        "      U_t = Unew\n",
        "      B_t = Bnew\n",
        "    Y_t = torch.zeros(m, q,dtype = datatype)\n",
        "    for k in range(q):\n",
        "      Y_t[:,k]    = torch.matmul(A[:,:,k], torch.matmul(U_t,B_t[:,k]))\n",
        "    loss = (Y_t - Y0_t).norm()\n",
        "\n",
        "    return loss, U_t, B_t\n",
        "\n",
        "\n",
        "\n",
        "################################################################################# Above Understood #######################################################################################\n",
        "  def EnableSingleLayer(self,en_l):\n",
        "    self.thr[0].requires_grad = True\n",
        "    for t in range(maxIt):\n",
        "      self.step[t].requires_grad = False\n",
        "    self.step[en_l].requires_grad = True\n",
        "    #self.thr[en_l].requires_grad = True\n",
        "  def EnableLayers(self, num_l):\n",
        "    self.thr[0].requires_grad = True\n",
        "    for t in range(num_l):\n",
        "      self.step[t].requires_grad = True\n",
        "      #self.thr[t].requires_grad = True\n",
        "    for t in range(num_l,maxIt):\n",
        "      self.step[t].requires_grad = False\n",
        "      #self.thr[t].requires_grad = False\n",
        "\n",
        "\n",
        "## =================Training Scripts======================\n",
        "Nepoches_pre \t= 4\n",
        "Nepoches_full \t= 4\n",
        "lr_fac \t\t\t= 1.0\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t# basic learning rate\n",
        "\n",
        "net = MatNet()\n",
        "optimizers = []\n",
        "optimizer = optim.Adam({net.thr[0]},lr = lr_fac * 0.01)\n",
        "optimizers.append(optimizer)\n",
        "for i in range(1,maxIt):\n",
        "  optimizer = optim.Adam({net.step[i]},lr = lr_fac * 0.1)\t# optimizer for each layer\n",
        "  #optimizer.add_param_group({'params': [net.thr[i]], 'lr': lr_fac * 0.01})\n",
        "  optimizers.append(optimizer)\n",
        "\n",
        "## =================Layerwise Training======================\n",
        "start = time.time()\n",
        "\n",
        "for stage in range(1,maxIt):\t\t\t\t\t\t\t\t\t\t\t\t\t\t# in k-th stage, we train the k-th layer\n",
        "\n",
        "\t## Pre-training: only train the k-th layer\n",
        "\tprint('Layer ',stage,', Pre-training ======================')\n",
        "\tif(stage > 6):\n",
        "\t\tNepoches_full = 2\n",
        "\tfor epoch in range(Nepoches_pre):\n",
        "\t\tfor i in range(maxIt):\n",
        "\t\t\toptimizers[i].zero_grad()\n",
        "\n",
        "\n",
        "\t\tU0_t,B0_t,Y0_t, A = generate_problem(r,n,q)\n",
        "\t\tnet.EnableSingleLayer(stage)\n",
        "\t\tloss, U_t, B_t = net(Y0_t,  U0_t, B0_t,A, stage+1)\n",
        "\t\tloss.backward()\n",
        "\n",
        "\t\toptimizers[stage].step()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t\tif epoch % 2 == 0:\n",
        "\t\t\tprint(\"epoch: \" + str(epoch), \"\\t loss: \" + str(loss.item()))\n",
        "\n",
        "\t# Full-training: train 0~k th layers\n",
        "\tprint('Layer ',stage,', Full-training =====================')\n",
        "\tif stage == 0:\n",
        "\t\tcontinue\n",
        "\n",
        "\tfor epoch in range(Nepoches_full):\n",
        "\t\tfor i in range(maxIt):\n",
        "\t\t\toptimizers[i].zero_grad()\n",
        "\n",
        "\t\tU0_t,B0_t,Y0_t,A = generate_problem(r,n,q)\n",
        "\t\tnet.EnableLayers(stage+1)\n",
        "\t\tloss, U_t, B_t = net(Y0_t,  U0_t, B0_t, A, stage+1)\n",
        "\t\tloss.backward()\n",
        "\n",
        "\t\tfor i in range(stage+1):\n",
        "\t\t\toptimizers[i].step()\n",
        "\n",
        "\t\tif epoch % 2 == 0:\n",
        "\t\t\tprint(\"epoch: \" + str(epoch), \"\\t loss: \" + str(loss.item()))\n",
        "\n",
        "\n",
        "end = time.time()\n",
        "print(\"Training end. Time: \" + str(end - start))\n",
        "\n",
        "## =====================Save model to .mat file ========================\n",
        "result_stp = np.zeros((maxIt,))\n",
        "result_thr = np.zeros((maxIt,))\n",
        "result_thr \t= net.thr[0].data.cpu().numpy()\n",
        "for i in range(maxIt):\n",
        "  result_stp[i] \t= net.step[i].data.cpu().numpy()\n",
        "\n",
        "\n",
        "spath = 'LRPCA_alpha'+'.mat'\n",
        "sio.savemat(spath, {'ths':result_thr, 'step':result_stp})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "J2MmDsuD316L",
        "outputId": "1b2e3dc8-a36b-47f5-fec9-ef6f13f4dfc6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer  1 , Pre-training ======================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-67b81d5ec011>:33: UserWarning: torch.qr is deprecated in favor of torch.linalg.qr and will be removed in a future PyTorch release.\n",
            "The boolean parameter 'some' has been replaced with a string parameter 'mode'.\n",
            "Q, R = torch.qr(A, some)\n",
            "should be replaced with\n",
            "Q, R = torch.linalg.qr(A, 'reduced' if some else 'complete') (Triggered internally at ../aten/src/ATen/native/BatchLinearAlgebra.cpp:2426.)\n",
            "  U0_t, _ \t\t= torch.qr(torch.randn(n,r,dtype = datatype)/math.sqrt(n))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0 \t loss: 0.46746613581645446\n",
            "epoch: 2 \t loss: 0.374043391828708\n",
            "Layer  1 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.3365417842991157\n",
            "epoch: 2 \t loss: 0.2546720950988543\n",
            "Layer  2 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.15843484035930547\n",
            "epoch: 2 \t loss: 0.1342793161305323\n",
            "Layer  2 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.1281863899488382\n",
            "epoch: 2 \t loss: 0.15272693914903157\n",
            "Layer  3 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.06620876698806762\n",
            "epoch: 2 \t loss: 0.06748732717566594\n",
            "Layer  3 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.05734098622509836\n",
            "epoch: 2 \t loss: 0.0773443560418312\n",
            "Layer  4 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.03921488220831694\n",
            "epoch: 2 \t loss: 0.03300717673873461\n",
            "Layer  4 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.031142103180006653\n",
            "epoch: 2 \t loss: 0.03685174565870248\n",
            "Layer  5 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.022121869839748592\n",
            "epoch: 2 \t loss: 0.017249657282639882\n",
            "Layer  5 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.018910425988002445\n",
            "epoch: 2 \t loss: 0.02395444846759116\n",
            "Layer  6 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.010282935276431182\n",
            "epoch: 2 \t loss: 0.010886491809757915\n",
            "Layer  6 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.007586936830590376\n",
            "epoch: 2 \t loss: 0.008053236827548122\n",
            "Layer  7 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.005997746262848515\n",
            "epoch: 2 \t loss: 0.0064507351359293\n",
            "Layer  7 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.005762983499605485\n",
            "Layer  8 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.0053767399693294814\n",
            "epoch: 2 \t loss: 0.003152746199416112\n",
            "Layer  8 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.003924219649844954\n",
            "Layer  9 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.0015128790100318941\n",
            "epoch: 2 \t loss: 0.0026983476553341323\n",
            "Layer  9 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.00275065541824166\n",
            "Layer  10 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.0022621411735774034\n",
            "epoch: 2 \t loss: 0.0017098897032160117\n",
            "Layer  10 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.002535607210917292\n",
            "Layer  11 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.0008000988011173156\n",
            "epoch: 2 \t loss: 0.001278616567778874\n",
            "Layer  11 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.000948545190718289\n",
            "Layer  12 , Pre-training ======================\n",
            "epoch: 0 \t loss: 0.0004468076575270132\n",
            "epoch: 2 \t loss: 0.0005484960843049457\n",
            "Layer  12 , Full-training =====================\n",
            "epoch: 0 \t loss: 0.00046581129121065997\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-67b81d5ec011>\u001b[0m in \u001b[0;36m<cell line: 133>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnableLayers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mU_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY0_t\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mU0_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB0_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_thr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GBwMHyNnFaH",
        "outputId": "996d5de9-1b37-4e85-aac4-c576e2958400"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(0.51669041)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result_stp\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGrCKILI34Cy",
        "outputId": "dd0e7107-ffb6-417f-fb14-4297d83482fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.5       , 1.93514147, 0.80864945, 0.60448286, 0.88496346,\n",
              "       1.43840961, 0.59636465, 2.1705129 , 1.76584915, 0.40185209,\n",
              "       0.39694755, 0.46756304, 2.16777527, 2.23668645, 2.07449664,\n",
              "       1.83732582, 0.37635988, 1.4958818 , 0.6621005 , 0.01929649,\n",
              "       0.32267303, 0.25271242, 1.19656133, 1.94216878, 1.82703888,\n",
              "       1.07353684, 0.98512214, 0.53016262, 0.45006827, 0.7629641 ,\n",
              "       0.71776654, 0.60472756, 0.84449897, 0.80453575, 0.82170544,\n",
              "       0.72900139, 0.77497536, 0.68617869, 0.76580305, 0.61025142,\n",
              "       0.59471921, 0.56268884, 0.55175302, 0.55308136, 0.75027799,\n",
              "       0.52572224, 0.52085304, 0.54441848, 0.52629019, 0.50931355,\n",
              "       0.51974804, 0.50695936, 0.50554592, 0.50342293, 0.50486421,\n",
              "       0.50266811, 0.50171058, 0.5043831 , 0.50129951, 0.50122675,\n",
              "       0.50130459, 0.50115624, 0.50119468, 0.50092603, 0.50119845,\n",
              "       0.50062525, 0.5004605 , 0.50035924, 0.5003906 , 0.50030337,\n",
              "       0.50033208, 0.500334  , 0.50019408, 0.50018676, 0.50017687,\n",
              "       0.50019466, 0.50019018, 0.50022278, 0.50016917, 0.50017942,\n",
              "       0.50018521, 0.50016976, 0.50017237, 0.50020021, 0.50021745,\n",
              "       0.50019472, 0.50022799, 0.50023514, 0.50000171, 0.50000199,\n",
              "       0.50000189, 0.50000195, 0.50000058, 0.5000035 , 0.50000015,\n",
              "       0.50000271, 0.50000013, 0.50000048, 0.50000001, 0.50000042])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}